%pdflatex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                    Included packages                 %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Included by IEEE:
% \documentclass[journal]{IEEEtran}
% \usepackage{times}

% % numbers option provides compact numerical references in the text. 
% \usepackage[numbers]{natbib}
% \usepackage{multicol}
% \usepackage[bookmarks=true]{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Included by science:
\documentclass[12pt]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% http://www.sciencemag.org/authors/preparing-manuscripts-using-latex 
% This package should properly format in-text
% reference calls and reference-list numbers.

%\usepackage{scicite}
\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.
% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm
%
\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%   Additional packages:

\usepackage[bookmarks=true]{hyperref}
\usepackage{pdfsync}
\usepackage[dvipsnames,table]{xcolor}
\usepackage{mathtools}
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
%\usepackage[final]{pdfpages} % for including pdfs
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
%\usepackage{endfloat}
\usepackage{amsmath}
\usepackage{nccmath} % for fleqn
\usepackage{algorithm}
\usepackage{pbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Macros:

\newcommand{\separator}{ \noindent \rule{\columnwidth}{1pt} }
\newenvironment{old}{\color{Maroon} \separator \textbf{[\textit{Old:}]} }{\ignorespacesafterend \separator}
\newenvironment{new}{\color{Blue} \separator \textbf{[\textit{New:}]} }{\ignorespacesafterend \separator}
\newenvironment{notes}{\color{Gray} \separator}{\ignorespacesafterend \separator}

% For making things invisible during double-blind review. Put "#1" in the
% the braces to make the text appear later.:
\newcommand{\doubleBlind}[1]{} 

% For marking Todos and changes
\newcommand{\TODO}[1]{ {\bf \textcolor{red}{TODO:} #1 }}
\newcommand{\abj}[1]{\textcolor{blue}{#1}}
\newcommand{\dbj}[1]{\textcolor{blue}{\sout{#1}}}
\newcommand{\cbj}[2]{\textcolor{blue}{\sout{#1}}\textcolor{blue}{~#2}}
%\newcommand{\abt}[1]{\textcolor{magenta}{#1}}
% Handy commands
\newcommand{\lt}{{\tt True}}
\newcommand{\lf}{{\tt False}}
\newcommand{\ltnsp}{{\tt True}}
\newcommand{\lfnsp}{{\tt False}}
\newtheorem{definition}{Definition}
\DeclareMathOperator{\F}{\rotatebox[origin=c]{45}{$\Box$}}
\DeclareMathOperator{\X}{\bigcirc}
\DeclareMathOperator{\G}{\Box}
\newcommand{\LTLG}{\G}
\newcommand{\LTLF}{\F}
\newcommand{\LTLX}{\X}

\newfloat{spec}{thb}{lop} %{thb}{lop}
\floatname{spec}{Specification}
\makeatletter
\newcommand{\leqnomode}{\tagsleft@true}
\newcommand{\reqnomode}{\tagsleft@false}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% More science preamble:
% Include your paper's title here
\title{An Integrated System for Perception-Driven Autonomy with Modular Robots} 


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.
\author
{Jonathan Daudelin\textsuperscript{*}, Gangyuan Jing\textsuperscript{*}, Tarik Tosun\textsuperscript{*}, \\ Mark Yim, Hadas Kress-Gazit, and Mark Campbell\\
\\
\normalsize{Daudelin, Jing, Kress-Gazit, and Campbell: Department of Mechanical and Aerospace Engineering,}\\
\normalsize{Cornell University, Ithaca NY, USA}\\
\normalsize{Tosun and Yim: Department of Mechanical Engineering and Applied Mechanics, }\\
\normalsize{University of Pennsylvania, Philadelphia PA, USA}\\
\\
\normalsize{\textsuperscript{*} J. Daudelin, G. Jing, and T. Tosun contributed equally to this work.} \\
\normalsize{Address correspondence to: Jonathan Daudelin, jd746@cornell.edu} \\
}

% Include the date command, but leave its argument blank.

\date{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                     Main document                        %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
%\IEEEoverridecommandlockouts

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Old IEEE title stuff:
% \bstctlcite{IEEEexample:BSTcontrol} % for automatic et al in bibliography

% \title{An Integrated System for Perception-Driven Autonomy with Modular Robots}
% \author{Jonathan~Daudelin\textsuperscript{*},~\IEEEmembership{Member,~IEEE,}
%         Gangyuan~Jing\textsuperscript{*},~\IEEEmembership{Member,~IEEE,}
%         Tarik~Tosun\textsuperscript{*},~\IEEEmembership{Member,~IEEE,}
%         Mark~Yim,~\IEEEmembership{Member,~IEEE,}
%         Hadas~Kress-Gazit,~\IEEEmembership{Senior Member,~IEEE,}
%         and~Mark~Campbell,~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space
% \thanks{J.Daudelin, G. Jing, M. Campbell, and H. Kress-Gazit are with the Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca,
% NY, 14850.}% <-this % stops a space
% \thanks{T. Tosun and M. Yim are with the Mechanical Engineering and Applied Mechanics Department, University of Pennsylvania, Philadelphia, PA, 19104.}% <-this % stops a space
% \thanks{\textsuperscript{*}J. Daudelin, G. Jing, and T. Tosun contributed equally to this work.}}


% \maketitle

% \begin{abstract}

% The theoretical ability of modular robots to reconfigure in response to complex tasks in \textit{a priori} unknown environments has frequently been cited as an advantage, but has never been experimentally demonstrated.
% For the first time, we present a system that integrates perception, high-level mission planning, and modular robot hardware, allowing a modular robot to autonomously reconfigure in response to an \textit{a priori} unknown environment in order to complete high-level tasks.
% Three hardware experiments validate the system, and demonstrate a modular robot autonomously exploring, reconfiguring, and manipulating objects to complete high-level tasks in unknown environments.

% %We present an integrated system architecture that allows self-reconfigurable modular robots to autonomously perform high-level tasks in unknown environments, using perception and reconfiguration to recognize and adapt to environment obstacles and constraints.  By performing three hardware experiments, we demonstrate that our implementation of the system is capable of autonomously completing different object manipulation tasks in different environments without human changes to the system.

% We present system architecture, software and hardware in a general framework that enables modular robots to solve tasks in unknown environments using autonomous, reactive reconfiguration. The physical robot is composed of modules that support multiple robot configurations. An onboard 3D sensor provides information about the environment and informs exploration, reconfiguration decision making and feedback control.  A centralized high-level mission planner uses information from the environment and the user-specified task description to autonomously compose low-level controllers to perform locomotion, reconfiguration, and other behaviors. A novel, centralized self-reconfiguration method is used to change robot configurations as needed.
% %This is the first modular robot system that uses perception-driven reconfiguration to intelligently adapt to \textit{a priori} unknown environments to perform complex tasks.

% \end{abstract}

% \IEEEpeerreviewmaketitle
% End of IEEE stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Science titling
% Double-space the manuscript.
% \baselineskip24pt
% Make the title.
\maketitle 

\part*{Figures and Tables}

%     _______
%    / ____(_)___ ___  __________  ______
%   / /_  / / __ `/ / / / ___/ _ \/ ___(_)
%  / __/ / / /_/ / /_/ / /  /  __(__  )
% /_/   /_/\__, /\__,_/_/   \___/____(_)
%         /____/

%Results Figures
\begin{figure}[H]
  % Map Figure
  \begin{subfigure}{0.5\columnwidth}
  \begin{center}
  \includegraphics[height=1.5in]{images/RSSMap.png}
  \caption{Diagram of Demonstration I environment}
  \label{fig:map}
  \end{center}
  \end{subfigure}
  % 
  % Octomap figure
  \begin{subfigure}{0.5\columnwidth}
  \begin{center}
  \includegraphics[height=1.5in]{images/map4.jpg}
  \caption{Volumetric map of environment 1 built by visual SLAM}
  \label{fig:octomap}
  \end{center}
  \end{subfigure}
  %
  % Environments and tasks table:
  \newcommand{\Lwidth}{0.4\columnwidth}
  \newcommand{\Rwidth}{0.4\columnwidth}
  %\newcommand{\Tbuffer}{-2cm}
  \newcommand{\Rboxheight}{-0.5\height}
  \setlength{\tabcolsep}{4pt} %reduces horizontal padding in table. 
  %\renewcommand{\arraystretch}{5}
  %\begin{table}
  \begin{subfigure}{\columnwidth}
  \centering
  \begin{tabular}{|c|c|}
  \hline
   & \vspace{-5pt}\\

  \textbf{Environment Setup} & \textbf{Task Description}\\

  \hline
   & \vspace{-5pt}\\
   
  \raisebox{\Rboxheight}{\includegraphics[width=0.3\columnwidth]{images/overhead_starting.jpg}}
  %
  & \pbox{\Rwidth}{\textbf{Demonstration I:} Explore environment to find all pink or green objects and blue dropoff zone. Deliver all objects to dropoff zone.}\\

   & \vspace{-5pt}\\
  \hline
   & \vspace{-5pt}\\
   
  \raisebox{\Rboxheight}{\includegraphics[width=0.3\columnwidth]{images/stairs_explore_overhead.jpg}}
  %
  & \pbox{\Rwidth}{\textbf{Demonstration II:} Explore environment to find mailbox, then deliver a circuit to the box.}\\

   & \vspace{-5pt}\\
  \hline
   & \vspace{-5pt}\\
   
  \raisebox{\Rboxheight}{\includegraphics[width=0.3\columnwidth]{images/stamp_explore_overhead.jpg}}
  %
  & \pbox{\Rwidth}{\textbf{Demonstration III:} Explore environment to find package, then place a stamp on the package.}\\

   & \\
  \hline
  \end{tabular}
  \caption{Environments and tasks for hardware demonstrations}
  \label{table:task-compare}
  %\end{table}
  \end{subfigure}
  \setlength{\tabcolsep}{6pt} % resetting to default
  %
\caption{Environments and Tasks for Demonstrations}
\label{fig:envs}
\end{figure}

\begin{figure}[H]
%
  \begin{subfigure}{\columnwidth}
  % Demonstration 1 Figure
  \begin{small}
  \begin{tabular}{c c c}
    \pbox{0.32\textwidth}{
        \includegraphics[width=0.32\textwidth]{images/overhead_starting.jpg}
        1. Environment and robot starting location} & 
    \pbox{0.32\textwidth}{
        \includegraphics[width=0.32\textwidth]{images/exploration.jpg}
        2. Exploring  while searching for objects} &
    \pbox{0.32\textwidth}{
        \includegraphics[width=0.32\textwidth]{images/reconfiguration.png}
        3. Reconfiguring to retrieve pink object}
    \\
    \pbox{0.32\textwidth}{
        \includegraphics[width=0.32\textwidth]{images/pink_retrieval.png}
        4. Retrieving pink object} &
        % \label{fig:pink_grab}
    \pbox{0.32\textwidth}{
        \includegraphics[width=0.32\textwidth]{images/dropoff.jpg}
        5. Depositing an object in the drop-off zone} &
        % \label{fig:dropoff}
    \pbox{0.32\textwidth}{
        \includegraphics[width=0.32\textwidth]{images/green_retrieval.jpg}
        %\label{fig:objb}
        6. Retrieving green object}
  \end{tabular}
  \end{small}
        \caption{Phases of Demonstration I.}
        %\label{fig:demo}
  \end{subfigure}
  %
  % Demonstration 2 and 3 Figure
  \begin{subfigure}[t]{\columnwidth}
    \centering
    \begin{small}
    \begin{tabular}{c c}
    \pbox{0.45\textwidth}{
      \includegraphics[width=0.45\textwidth]{images/stairs_reconfig.jpg}
      %\label{fig:obja}
      1. Reconfiguring to climb stairs} &
    \pbox{0.45\textwidth}{
        \includegraphics[width=0.45\textwidth]{images/stairs_climb.jpg}
        %\label{fig:objb}
        2. Successful circuit delivery} 
    \\ 
    \pbox{0.45\textwidth}{
        \includegraphics[width=0.45\textwidth]{images/stamp_reconfig.jpg}
        %\label{fig:objb}
        1. Reconfiguring to place stamp} &
    \pbox{0.45\textwidth}{
        \includegraphics[width=0.45\textwidth]{images/stamp_placing.jpg}
        %\label{fig:objb}
        2. Successful stamp placement}
    \end{tabular}
    \end{small}
      \caption{Demonstrations II and III.}
      %\label{fig:exps}
  \end{subfigure}  
  \caption{Demonstrations 1, 2, and 3}
  \label{fig:experiments}
\end{figure}

% System Overview Figure
\begin{figure}[H]
\begin{center}
%\includegraphics[width=0.4\textwidth]{images/overview.png}
\includegraphics[width=0.8\columnwidth]{images/RSS17FlowchartV5.png}
\caption{System Overview Flowchart}
\label{fig:overview}
\end{center}
\end{figure}

% SMORES-EP module DoF picture
% Sensor Module Figure
\begin{figure}[H]
  \begin{subfigure}{0.5\columnwidth}
  \begin{center}
  \includegraphics[height=1.5in]{images/smores_dof.pdf}
  \end{center}
  \caption{SMORES-EP module}
  \label{fig:smores-module}
  \end{subfigure}
  %
  \begin{subfigure}{0.5\columnwidth}
  \begin{center}
  \includegraphics[width=0.3\textwidth]{images/sensor_module_new_labelled.jpg}
  \caption{Sensor Module with labelled components.  UP board and battery are inside the body.}
  \label{fig:sensor-module}
  \end{center}
  \end{subfigure}
  \caption{SMORES-EP Module and Sensor Module}
\end{figure}

\begin{figure}[H]
% Environment types figure
\begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \begin{subfigure}[t]{0.45\columnwidth}
        \includegraphics[width=\textwidth]{images/FreeEnvironment.png}
        %\label{fig:obja}
        \caption{\textbf{``free'}' environment}
    \end{subfigure} \ \ \ \ \ \
    \begin{subfigure}[t]{0.45\columnwidth}
        \includegraphics[width=\textwidth]{images/TunnelEnvironment.png}
        %\label{fig:objb}
        \caption{\textbf{``tunnel''} environment}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.45\columnwidth}
        \includegraphics[width=\textwidth]{images/HighFreeEnvironment.png}
        %\label{fig:objb}
        \caption{\textbf{``high''} environment}
    \end{subfigure} \ \ \ \ \ \
    \begin{subfigure}[t]{0.45\columnwidth}
        \includegraphics[width=\textwidth]{images/LedgeEnvironment.png}
        %\label{fig:objb}
        \caption{\textbf{``stairs''} environment}
    \end{subfigure}
      \caption{Environment characterization types.}
      \label{fig:characters}
\end{subfigure}
%
% Characterization method figure
\begin{subfigure}{0.5\columnwidth}
\begin{center}
\includegraphics[width=\columnwidth]{images/characterization.png}
\caption{An example of a \textbf{tunnel} environment characterization. Yellow grid cells are occupied, light blue cells are unreachable resulting from bloating obstacles.}
\label{fig:characterization}
\end{center}
\end{subfigure}
\caption{Environment Characterization}
\end{figure}

% Reconfiguration Figure
\begin{figure}[H]
\begin{center}
  \includegraphics[width=0.32\columnwidth]{images/reconf_start.jpg}
  \includegraphics[width=0.32\columnwidth]{images/reconf_motion.png}
  \includegraphics[width=0.32\columnwidth]{images/reconf_end.jpg}
  \caption{Module movement during reconfiguration. Left: initial configuration (``Car''). Middle: module movement, using AprilTags for localization. Right: final configuration (``Proboscis'').}
  \label{fig:reconf}
\end{center}
\end{figure}

% Controller Automaton Figure
\begin{figure}[H]
\begin{center}
\begin{subfigure}[t]{\columnwidth}
\centering
    \includegraphics[width=\textwidth]{images/spec.png}
\caption{The specification for dropping an object to the mailbox.}
\label{fig:spec}
\end{subfigure}

\begin{subfigure}[t]{\columnwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{images/autSimple.png}
\caption{The synthesized controller. A proposition with ``!'' has a value of \lf{}, and \lt{} otherwise.}
\label{fig:autSimple}
\end{subfigure}

\label{fig:specAut}
\caption{A task specification with the synthesized controller.}
\end{center}
\end{figure}


%\begin{spec}[h!]
%\caption{Drop an object to the mailbox}
%\label{spec:experiment}
%\vspace{-0.1cm}
%\small\setlength{\jot}{0pt}
%\begin{fleqn}[3pt]
%\leqnomode
%\begin{subequations}
%\renewcommand{\theequation}{\arabic{equation}} 
%\makeatletter
%\renewcommand\tagform@[1]{\maketag@@@{\ignorespaces#1\unskip\@@italiccorr}}
%\makeatother
%\hskip-10cm
%\begin{alignat}{2}
%&\text{do {\bf explore} if and only if the robot is not sensing {\bf mailBox}}&& \notag \\
%&\text{do {\bf driveToMailBox} if and only if the robot is sensing ({\bf mailBox} and not {\bf arrived})}&& \notag \\
%&\text{do {\bf drop} if and only if the robot is sensing ({\bf arrived} and {\bf mailBox})}&& \notag
%\end{alignat}
%\end{subequations}
%\end{fleqn}
%\vspace{-0.4cm}
%\end{spec}

%   ______      __    __
%  /_  __/___ _/ /_  / /__  ______
%   / / / __ `/ __ \/ / _ \/ ___(_)
%  / / / /_/ / /_/ / /  __(__  )
% /_/  \__,_/_.___/_/\___/____(_)

% Library table
\begin{table}[H]
\centering
\begin{tabular}{ |c|c|c| } 
 \hline
 \multirow{2}{6em}{Configuration} & Behavior & Environment \\
 & properties & Types \\
 \hline
 \multirow{3}{*}{Car} & \textbf{pickUp} & ``free'' \\\cline{2-3}
  & \textbf{drop} & ``free'' \\\cline{2-3}
  & \textbf{drive} & ``free''\\ \hline
 \multirow{3}{*}{Proboscis} & \textbf{pickUp} & ``tunnel'' or ``free''\\ \cline{2-3}
  & \textbf{drop} &``tunnel'' or ``free'' \\ \cline{2-3}
  & \textbf{highReach} & ``high''\\ \hline
 Scorpion & \textbf{drive} & ``free''\\ \hline
 \multirow{3}{*}{Snake} & \textbf{climbUp} & ``stairs''\\ \cline{2-3}
  & \textbf{climbDown} & ``stairs''\\ \cline{2-3}
  & \textbf{drop} & ``stairs'' or ``free''\\
 \hline
\end{tabular}
\caption{A library of robot behaviors}
\label{table:1}
\end{table}

% Demonstration Failure Table
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Reason of failure} & \textbf{Number of times} & \textbf{Percentage}\\ 
\hline
Hardware Issues & 10 & 41.7\% \\ 
\hline
Navigation Failure & 3 & 12.5\% \\ 
\hline
Perception-Related Errors & 6 & 25\% \\ 
\hline
Network Issues & 1 & 4.2\% \\ 
\hline
Human Error & 4 & 16.7\% \\ 
\hline
\end{tabular}
\caption{Reasons for demonstration failure.}
\label{table:errors}
\end{table}

%    _____                   __                          __
%   / ___/__  ______  ____  / /__  ____ ___  ___  ____  / /_____ ________  __
%   \__ \/ / / / __ \/ __ \/ / _ \/ __ `__ \/ _ \/ __ \/ __/ __ `/ ___/ / / /
%  ___/ / /_/ / /_/ / /_/ / /  __/ / / / / /  __/ / / / /_/ /_/ / /  / /_/ /
% /____/\__,_/ .___/ .___/_/\___/_/ /_/ /_/\___/_/ /_/\__/\__,_/_/   \__, /
%           /_/   /_/                                               /____/
\part*{Supplementary Materials}

%     ____       __      __           __   _       __           __
%    / __ \___  / /___ _/ /____  ____/ /  | |     / /___  _____/ /__
%   / /_/ / _ \/ / __ `/ __/ _ \/ __  /   | | /| / / __ \/ ___/ //_/
%  / _, _/  __/ / /_/ / /_/  __/ /_/ /    | |/ |/ / /_/ / /  / ,<
% /_/ |_|\___/_/\__,_/\__/\___/\__,_/     |__/|__/\____/_/  /_/|_|

\section{Additional Commentary on Related Work}\label{sec:related-work}
%
%
%%% Paragraph from intro
% The traditional approach to achieving flexible
% robots is to build  monolithic systems that are highly capable, but also highly
% complex (\emph{e.g.} large humanoids).  Self-reconfigurability is an elegant,
% scalable alternative: since the shape of the robot is not fixed, each individual
% task can be solved with a morphology that is only as complicated as it needs
% to be.
%%%

% millibot
Here we provide a more detailed overview of prior work in MSRR systems.  These systems provide partial sets of the capabilities of our system.
 
The Millibot system demonstrated mapping when operating as a swarm. Certain members of the swarm are designated as ``beacons,'' and have known locations. The autonomy of the Millibot swarm is limited: a human operator makes all high-level decisions, and is responsible for navigation using a GUI \cite{Grabowski2000}.

% swarm-bots
The Swarm-Bots system has been applied in exploration \cite{Dorigo2005} and collective manipulation \cite{Mondada2005} scenarios.  Like the Millibots, some members of the swarm act as ``beacons'' that are assumed to have known location during exploration.  In a collective manipulation task, Swarm-Bots have limited autonomy, with a human operator specifying the location of the manipulation target and the global sequence of manipulation actions.

%Swarm-Bots have demonstrated the capability to use self-assembly to solve low-level tasks, such as crossing a gap \cite{gross2006autonomous} or ascending a small hill \cite{o2010self}.  In these scenarios, ground proximity sensors and tilt sensors are used to trigger self-assembly.  In our work, 3D map data is used to characterize the environment, and the system autonomously selects specific morphologies that are appropriate to the task and environment. 
%
In \cite{o2010self}, Swarm-Bots demonstrate swarm self-assembly to climb a hill.  Robots exhibit phototaxis, with the goal of moving toward a light source.  When robots detect the presence of a hill (using tilt sensors), they aggregate to form a random connected structure to collectively surmount the hill. A similar strategy is employed to cross holes in the ground.  In each case, the swarm of robots is loaded with a single self-assembly controller specific to an \textit{a priori} known obstacle type (hill or hole).  The robots do not self-reconfigure between specific morphologies, but rather self-assemble, beginning as a disconnected swarm and coming together to form a random connected structure.  In our work, a modular robot completes high-level tasks by autonomously self-reconfiguring between specific morphologies with different capabilities.  Our system differentiates between several types of environments using RGB-D data, and may choose to use different morphologies to solve a given high-level task in different environments.    
 
%swarmanoid
The swarmanoid project (successor to the swarm-bots), uses a heterogeneous swarm of ground and flying robots (called ``hand-'', ``foot-'', and ``eye-'' bots) to perform exploration and object retrieval tasks  \cite{Dorigo2013}. Robotic elements of the swarmanoid system connect and disconnect to complete the task, but the decision to take this action is not made autonomously by the robot in response to sensed environment conditions. While the location of the object to be retrieved is unknown, the method for retrieval is known and constant.

% CKbot, Conro, MTRAN
Self-reconfiguration has been demonstrated with several other modular robot systems. CKbot, Conro, and MTRAN have all demonstrated the ability to join disconnected clusters of modules together \cite{Yim2007, Rubenstein2004,Murata2006}. In order to align, Conro uses infra-red sensors on the docking faces of the modules, while CKBot and MTRAN use a separate sensor module on each cluster.  In all cases, individual clusters locate and servo towards each other until they are close enough to dock. These experiments do not include any planning or sequencing of multiple reconfiguration actions in order to create a goal structure appropriate for a task.  Additionally,  modules are not individually mobile, and mobile clusters of modules are limited to slow crawling gaits.  Consequently, reconfiguration is very time consuming, with a single connection requiring 5-15 minutes.

% TEMP
Other work has focused on reconfiguration planning.  Paulos et al. present a system in which self-reconfigurable modular boats self-assemble into prescribed floating structures, such as a bridge \cite{Paulos2015}.  Individual boat modules are able to move about the pool, allowing for rapid reconfiguration.  In these experiments, the environment is known and external localization is provided by an overhead AprilTag system. 

MSRR systems have demonstrated the ability to accomplish low-level tasks such as various modes of locomotion \cite{Yim1994}.
Recent work includes a system which integrates many low-level capabilities of a MSRR system in a design library, and accomplishes high-level user-specified tasks by synthesizing elements of the library into a reactive state-machine \cite{Jing2016}. This system demonstrates autonomy with respect to task-related decision making, but is designed to operate in a fully known environment with external sensing.

% Traditional systems and wrapup statement
Our system goes beyond existing work by using self-reconfiguration capabilities of an MSRR system to take autonomy a step further.  The system uses perception of the environment to inform the choice of robot configuration, allowing the robot to adapt its abilities to surmount challenges arising from \textit{a priori} unknown features in the environment. Through hardware demonstrations, we show that autonomous self-reconfiguration allows our system to adapt to the environment to complete complex tasks.


\section{Environment Characterization}
%
When the system recognizes an object in the environment, the characterization algorithm evaluates the 3D information in the object's surroundings. It creates an occupancy grid around the object location, and denotes all grid cells within a robot-radius of obstacles as unreachable (illustrated in Figure~\ref{fig:characterization}). The algorithm then selects the closest reachable point to the object within $20^o$ of the robot's line of sight to the object. If the distance from this point to the object is greater than a threshold value and the object is on the ground, the algorithm characterizes the environment as a ``tunnel''. If above the ground, it characterizes the environment as a ``stairs'' environment. If the closest reachable point is under the threshold value, the system assigns a ``free'' or ``high'' environment characterization, depending on the height of the colored object.

Based on the environment characterization and target location, the algorithm also returns a waypoint for the robot to position itself to perform its task (or to reconfigure, if necessary).  In Demonstration II, the environment characterization algorithm directs the robot to drive to a waypoint at the base of the stairs, which is the best place for the robot to reconfigure and begin climbing the stairs.

%    ______            _____          _____                 _ _____
%   / ____/___  ____  / __(_)___ _   / ___/____  ___  _____(_) __(_)_________
%  / /   / __ \/ __ \/ /_/ / __ `/   \__ \/ __ \/ _ \/ ___/ / /_/ / ___/ ___/
% / /___/ /_/ / / / / __/ / /_/ /   ___/ / /_/ /  __/ /__/ / __/ / /__(__  )
% \____/\____/_/ /_/_/ /_/\__, /   /____/ .___/\___/\___/_/_/ /_/\___/____/
%
\section{Library of Configurations and Behaviors}
\label{sec:configuration-specifics-supplement}
%
In this work, we use the architecture introduced in \cite{Jing2016}. We encode the full set of capabilities of the modular robot, such as driving and picking up items, in a library of robot configurations and behaviors.
To create robot configurations and behaviors, users can utilize our simulator toolbox VSPARC (Verification, Simulation, Programming And Robot Construction \footnote{\url{www.vsparc.org}}) presented in \cite{Jing2016}.
VSPARC allows users to design, simulate and test configurations and behaviors for the SMORES-EP robot system.
%The library currently has 57 robot configurations and 97 behaviors, which can be used to directly control the physical SMORES-EP robot to perform various actions.

Our implementation relies on a framework first presented in \cite{Jing2016}, which is summarized here.
A library entry is defined as $l = (C,B_C,P_b,P_e)$ where:
\begin{itemize}
\item $C$ is the robot \emph{configuration}, specified by the number of modules and the connected structure of the modules.
\item $B_C$ is a \emph{behavior} that $C$ can perform. A behavior is a controller that specifies commands for robot joints to perform a specific movement. 
\item $P_b$ is a set of \emph{behavior properties} that describes what $B_C$ does. 
\item $P_e$ is a set of \emph{environment types} that describe the environments in which this library entry is suitable. 
\end{itemize} 
%
To specify tasks at the high level, behavior properties $P_b$ are used to describe desired robot actions without explicitly specifying a configuration or behavior.
Environment types $P_e$ specify the conditions under which a behavior can be used.
This allows the high-level planner to match environment characterizations from the perception subsystem with configurations and behaviors that can perform the task in the current environment. 
In Demonstration II, when the environment characterization algorithm reports that the mailbox is located in a ``stairs''-type environment, the high-level planner queries the library for configurations that can climb stairs.  
Since the library indicates that current configuration is only capable of driving on flat ground, the high-level planner opts to reconfigure to the stair-climber configuration, and executes its \textbf{climbUp} behavior.

In \cite{Jing2016}, all robot behaviors are \textit{static} behaviors.
That is, once users create a behavior in VSPARC, joint values for each module are fixed and cannot be modified during behavior execution.
Static behaviors, such as a car with a fixed turning radius, do not provide enough maneuverability for the robot to navigate around unknown environment.
In this work, we expand the type of behaviors in the library by using \textit{parametric} behaviors, which were first introduced in \cite{JingAURO2017}.
Parametric behaviors have joint commands that can be altered during run-time, and therefore allow a wider range of motions.
For example, a parametric behavior for a car configuration can be a driving action with two parameters: turning angle and driving velocity.  
The system associates a parametric behavior with a program that generates values of joint commands based on environment information and current robot tasks.
Based on the sensed environment, the perception and exploration subsystem (Section~\ref{sec:exploration}) can generate a collision-free path, which is used to calculate real-time velocity for the robot.
The system then converts the robot velocity to joint values in parametric behaviors at run-time.

To provide an illustrative example, this paper discusses two configurations and their capabilities in detail.
The ``Car'' configuration shown in Figure~\ref{fig:experiments}a-5 is capable of picking
up and dropping objects in a ``free'' environment. In addition, the ``Car'' configuration can locomote on flat terrain. It uses a parametric differential drive behavior to convert a desired velocity vector into motor commands (\textbf{drive} in Table \ref{table:1}).

The ``Proboscis'' configuration shown in Figure~\ref{fig:experiments}a-4 has
a long arm in front, and is suitable for reaching between obstacles in a narrow ``tunnel'' environment to grasp objects or reaching up in a ``high'' environment to drop items.
However, the locomotion behaviors available for this configuration are limited to forward/backward motion, making it unsuitable for general navigation.

This library-based framework allows users to express desired robot actions in an abstract way by specifying behavior properties. For example, if a task specifies that the robot should execute a behavior with the \textbf{drop} property, the system could choose to use either the Car or Proboscis configurations to perform the action, since both have behaviors with the \textbf{drop} property.
The decision of which configuration to use is made during task execution, based on the sensed environment.
For example, if the perception system reports that the environment is of type ``tunnel'', the Proboscis configuration will be used, because the library indicates that it can be used in ``tunnel''-type environments while the Car cannot.

\section{High-Level Planner}
\label{sec:high-level-supplement}

In order to generate controllers from high-level task specifications, we first abstract the robot and environment status as a set of Boolean propositions.
In Demonstration II, the robot action \textbf{drop} is \lt{} if the robot is currently dropping an object in the mailbox (and \lf{} otherwise) and the environment proposition \textbf{mailBox} is \lt{} if the robot is currently sensing a mailbox (and \lf{} otherwise).
Moreover the proposition \textbf{explore} encodes whether or not the robot is currently searching for the target, the mailbox in this case.

%A wide range of reactive robotic tasks can be specified in terms of these propositions.
By using a library of robot configurations and behaviors as well as environment characterization tools, we can map these high-level abstraction to low-level sensing programs and robot controllers.
As discussed in Section~\ref{sec:configuration-specifics-supplement}, the user specifies high-level robot actions in terms of behavior properties from the library. 
In Demonstration II, our system can choose to do a drop action by executing any behavior from the library which has the behavior property \textbf{drop}, and which also satisfies the current ``stairs''-type environment. If the current robot configuration cannot execute an appropriate behavior, the robot will reconfigure to a different configuration that can.  In this way, the system autonomously chooses to implement  \textbf{drop}  appropriately in response to the sensed environment.
Our system evaluates propositions related to the state of the environment using perception and environment characterization tools in Section~\ref{sec:exploration}. For example, users can map the proposition \textbf{mailBox} to the color tracking function in our perception subsystem, which assign the value \lt{} to \textbf{mailBox} if and only if the robot is currently seeing a mailbox with the onboard camera.
The system treats propositions, such as \textbf{explore}, that require the robot to navigate in the workspace differently from the other simple robot actions, such as \textbf{drop}.
In this example, users can map \textbf{explore} to behavior property \textbf{drive}, which represents a set of parametric behaviors as discussed in Section~\ref{sec:configuration-specifics-supplement}.
In order to obtain joint values for behaviors at run-time, a path planner in the perception and planning subsystem (Section~\ref{sec:exploration}) takes into account the robot goal as well as the current environment information from the perception subsystem, and generates a collision-free path for the robot to follow.
Our system then converts this path to joint values, which are used to execute the \textbf{drive} behaviors.

%in Specification~\ref{spec:experiment}, the proposition \textbf{arrived} is \lt{} if the robot arrives at its target location, which is determined by consulting the robot's position in the map generated by SLAM. The proposition \textbf{dropoffZone} is \lt{} if the robot is currently sensing a drop-off zone, which is determined by consulting both the map and color tracking functions.

%The user specifies high-level robot actions in terms of behavior properties from the library.  For example, Line 7 in Specification~\ref{spec:experiment} specifies that under certain conditions, the robot should do \textbf{pickUp}.  As discussed in Section~\ref{sec:configuration-specifics}, our system can choose to do \textbf{pickUp} by executing any behavior from the library which has the behavior property \textbf{pickUp}, and which also satisfies the current environment characteristics. If the current robot configuration cannot execute an appropriate behavior, the robot will reconfigure to a different configuration that can.  In this way, the system autonomously chooses to implement  \textbf{pickUp}  appropriately in response to the sensed environment.

%Some robot actions, such as \textbf{driveExplore}, \textbf{driveToObject}, and \textbf{driveToDropoff}, requires the robot to navigate in the environment without colliding with obstacles.
%To achieve this, a goal pose is first determined by the controller and sent to a path planning program to generate a collision free path while considering the dynamics of the robot.
%The path is then converted to a sequence of robot velocity vectors and used as the input to control a parametric driving behavior in the library.
%In this work, we use the ROS navigation package\footnote{http://wiki.ros.org/navigation} to generate paths for a differential drive robot to control the ``Car'' configuration in the library.

%Propositions related to the state of the environment are evaluated using the perception tools described in Section~\ref{sec:exploration}. For example in Specification~\ref{spec:experiment}, the proposition \textbf{arrived} is \lt{} if the robot arrives at its target location, which is determined by consulting the robot's position in the map generated by SLAM. The proposition \textbf{dropoffZone} is \lt{} if the robot is currently sensing a drop-off zone, which is determined by consulting both the map and color tracking functions.

%\begin{spec}[h!]
%\caption{Search and move any object of interest to the drop-off zone}
%\label{spec:experiment}
%\vspace{-0.1cm}
%\small\setlength{\jot}{0pt}
%\begin{fleqn}[3pt]
%\leqnomode
%\begin{subequations}
%\renewcommand{\theequation}{\arabic{equation}} 
%\makeatletter
%\renewcommand\tagform@[1]{\maketag@@@{\ignorespaces#1\unskip\@@italiccorr}}
%\makeatother
%\hskip-10cm
%\begin{alignat}{2}
%&\text{{\bf 1.} {\bf carry} is set on {\bf pickUp} and reset on {\bf drop}}&& \notag \\
%&\text{{\bf 2.} if you are not activating ({\bf pickUp} or {\bf drop} or {\bf driveToObject}}&& \notag \\
%&\hspace{1cm}\text{or {\bf driveToDropoff} or {\bf carry}) then do {\bf driveExplore}}&& \notag \\
%&\text{{\bf 3.} if you are activating {\bf carry} and you are not sensing }&& \notag \\
%&\hspace{1cm}\text{{\bf dropoffZone} then do {\bf driveExplore}}&& \notag \\
%&\text{{\bf 4.} do {\bf driveToDropoff} if and only if you are activating {\bf carry} }&& \notag \\
%&\hspace{1cm}\text{and you are sensing {\bf dropoffZone}}&& \notag \\
%&\text{{\bf 5.} do {\bf driveToObject} if and only if you are not activating {\bf carry} }&& \notag \\
%&\hspace{1cm}\text{and you are sensing {\bf object}}&& \notag \\
%&\text{{\bf 6.} do {\bf drop} if and only if you were activating {\bf driveToDropoff } }&& \notag \\
%&\hspace{1cm}\text{and you are sensing {\bf arrived}}&& \notag \\
%&\text{{\bf 7.} do {\bf pickUp } if and only if you were activating {\bf driveToObject} }&& \notag \\
%&\hspace{1cm}\text{and you are sensing {\bf arrived}}&& \notag \\
%&\text{{\bf 8.} infinitely often not {\bf carry}}&& \notag
%\end{alignat}
%\end{subequations}
%\end{fleqn}
%\vspace{-0.4cm}
%\end{spec}


Our implementation employs the Linear Temporal Logic MissiOn Planning (LTLMoP) toolkit to automatically generate robot controllers from user-specified high-level instructions using synthesis \cite{DBLP:conf/iros/FinucaneJK10,DBLP:journals/trob/Kress-GazitFP09}.
The user describes the desired robot tasks with high-level specifications over the set of abstracted robot and environment propositions that are mapped to behavior properties from the library.
LTLMoP automatically converts the specification to logic formulas, which are then used to synthesize a robot controller that satisfies the given tasks (if one exists).
The controller is in the form of a finite state automaton, as shown in Figure~\ref{fig:autSimple}.
Each state specifies a set of high-level robot actions that need to be performed, and transitions between states include a set of environment propositions.
Note some of propositions are omitted in Figure~\ref{fig:autSimple} for clarity.
Execution of the high-level controller begins at the predefined initial state in the finite state automaton. In each iteration, LTLMoP determines the values of all environment propositions by calling the corresponding sensing program. Then, LTLMoP chooses the next state in the finite state machine by taking the transition that matches the current value of all environment propositions. 
In the next state, for each robot proposition LTLMoP chooses a behavior from the design library which satisfies both the behavior properties and current environment type.
For example, in Figure~\ref{fig:autSimple} we start in the top state and execute the \textbf{explore} program.
If  the robot senses a mailbox, the value of \textbf{mailBox} becomes \lt{} and therefore the next state is the bottom right state. We then stop the \textbf{explore} program and execute the \textbf{driveToMailBox} program.
We introduce additional constraints to the original task specifications to guarantee that there exist behaviors in the library to implement the synthesized controller.
Since self-reconfiguration is time-consuming, the controller chooses to execute the selected behavior using the current robot configuration whenever possible.
If the current configuration cannot execute the behavior, the controller instructs the robot to reconfigure to one that can, and if multiple appropriate configurations are available, the controller selects one at random.

% Controller Automaton Figure

%Since the synthesized high-level controller is a discrete finite state automaton, we need to implement it continuously in order to control the robot to satisfy the given task.
%Execution of the high-level controller begins at the predefined initial state in the finite state automaton. In each iteration, the values of all environment propositions are determined by calling the corresponding sensing program. Then, we determine the next state in the finite state machine by taking the transition that matches the current value of all environment propositions. 
%In the next state, the specified high level behavior properties are mapped to a behavior from the design library which satisfies both the behavior properties and current environment type.
%The system then maps the system proposition specified in the next state to its set of behavior properties, and selects a behavior from the design library which satisfies both the behavior properties and current environment properties.
%If the robot is not currently in a configuration capable of executing the selected behavior, the system commands the robot to reconfigure. Finally, the behavior is executed, and the program continues on to the next iteration.

%     ____                        _____                        __  _
%    / __ \___  _________  ____  / __(_)___ ___  ___________ _/ /_(_)___  ____
%   / /_/ / _ \/ ___/ __ \/ __ \/ /_/ / __ `/ / / / ___/ __ `/ __/ / __ \/ __ \
%  / _, _/  __/ /__/ /_/ / / / / __/ / /_/ / /_/ / /  / /_/ / /_/ / /_/ / / / /
% /_/ |_|\___/\___/\____/_/ /_/_/ /_/\__, /\__,_/_/   \__,_/\__/_/\____/_/ /_/
%                                   /____/
\section{Reconfiguration}
\label{sec:reconfiguration-supplement}
%
When the high-level planner decides to use a new configuration during a task, the robot must reconfigure. Our system architecture allows any method for reconfiguration, provided that the method requires no external sensing. SMORES-EP is capable of all three classes of modular self-reconfiguration (chain, lattice, and mobile reconfiguration) \cite{Davey2012,yim2003modular}.  We have implemented tools for mobile reconfiguration with SMORES-EP, taking advantage of the fact that individual modules can drive on flat surfaces as described in Section \ref{sec:hardware}.

Determining the relative positions of modules during mobile self-reconfiguration is an important challenge. 
%As discussed in Section~\ref{sec:related-work}, past systems have relied on offboard global positioning systems \cite{Paulos2015} or distributed approaches, in which sensors are mounted on each disconnected set of modules \cite{Yim2007}.  
In this work, the localization method is centralized, using a camera carried by the robot to track AprilTag fiducials mounted to individual modules.
As discussed in Section~\ref{sec:hardware}, the camera provides a view of a $0.75\text{m}\times0.5\text{m}$ area on the ground in front of the sensor module.  
Within this area, the localization system provides pose for any module equipped with an AprilTag marker to perform reconfiguration. 
%Within this area (which we call the \emph{reconfiguration zone}), any module equipped with an AprilTag marker can detach from the cluster, drive to another location, and reattach to the cluster, provided that both of its wheels were in contact with the ground in its starting position.% \TODO{Get accurate number for height and FoV.}

%\subsection{Reconfiguration Procedure}
Given an initial configuration and a goal configuration, the reconfiguration controller commands a set of modules to disconnect, move and reconnect in order to form the new topology of the goal configuration. 
The robot first takes actions to establish the conditions needed for reconfiguration by confirming that the reconfiguration zone is a flat surface free of obstacles (other than the modules themselves).
%If the robot is carrying an object, it drops the object outside of the reconfiguration zone.
The robot then sets its joint angles so that all modules that need to detach have both of their wheels on the ground, ready to drive.
Then the robot performs operations to change the topology of the cluster by detaching a module from the cluster, driving, and re-attaching at its new location in the goal configuration, as shown in Figure~\ref{fig:reconf}.
Currently, reconfiguration plans from one configuration to another are created manually and stored in the library. However the framework can work with existing assembly planning algorithms (\cite{Werfel2007,Seo2013}) to generate reconfiguration plans automatically.
Because the reconfiguration zone is free of obstacles, the controller compute collision-free paths offline and store them as part of the reconfiguration plan.
Once all module movement operations have completed and the goal topology is formed, the robot sets its joints to appropriate angles for the goal configuration to continue performing desired behaviors.

We developed several techniques to ensure reliable connection and disconnection during reconfiguration.  
When a module disconnects from the cluster, the electro-permanent magnets on the connected faces are turned off.  To guarantee a clean break of the magnetic connection, the disconnecting module bends its tilt joint up and down, mechanically separating itself from the cluster. During docking, accurate alignment is crucial to the strength of the magnetic connection \cite{tosun2016design}.  For this reason, rather than driving directly to its final docking location, a module instead drives to a pre-docking waypoint directly in front of its docking location.  At the waypoint, the module spins in place slowly until its heading is aligned with the dock point, and then drives in straight to attach. To guarantee a good connection, the module intentionally overdrives its dock point, pushing itself into the cluster while firing its magnets.
%
% Reconfiguration Figure

\end{document}
